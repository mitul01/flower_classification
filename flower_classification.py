# -*- coding: utf-8 -*-
"""flower_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GWDMjVzJ0M9gWKvQRkjaw3xsj-LE8BHJ
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""# Exploring Data"""

train=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competitions/Flower predictions/Train.csv')
test=pd.read_csv('/content/drive/My Drive/Colab Notebooks/Competitions/Flower predictions/Test.csv')

train.head()

train.describe()

train.isnull().sum()

plt.figure(figsize=(12,8))
sns.heatmap(train.corr(),annot=True)

"""# Combining categorical and num columns using PCA and MCA"""

pip install prince

import prince

y=train['Class']
train_no_target=train.drop('Class',axis=1)
flowers=pd.concat([train_no_target,test],axis=0)

categorical_cols= ['Area_Code','Locality_Code','Region_Code','Species']
numerical_cols = ['Height','Diameter']

mca = prince.MCA(n_components=2,random_state=202020).fit(flowers[categorical_cols])

train_no_target.loc[:,'MCA1'] = mca.transform(train_no_target[categorical_cols])[0]
test.loc[:,'MCA1']= mca.transform(test[categorical_cols])[0]

from sklearn.decomposition import PCA

pca= PCA(n_components=2,random_state=202020).fit(flowers[numerical_cols])
train_no_target.loc[:,'PCA1']= pca.transform(train_no_target[numerical_cols])[:,0]
test.loc[:,'PCA1']= pca.transform(test[numerical_cols])[:,0]

train_no_target.head()

train_no_target.corr()

flowers=pd.concat([train_no_target,test],axis=0)

"""# Aggregate Feature Creation

We are trying to create new columns using aggregate feature creation techniques. We will create columns like Area_Code_mean_height , Region_Code_max_diameter etc..
We can also create a new columns like No.of unique species in Area_Code.This helps to extract out much more meaningful
features from data.
"""

def agg_groupby(df,primary_key,operations,agg_cols):
  return df.groupby(primary_key).agg(operations).reset_index().rename(columns=agg_cols)

def left_join(df1,df2,primary_key):
  return df1.merge(df2,how='left',on=primary_key)

def aggregation(df,primary_key,operation,cols):
  for c in cols:
    df=left_join(df,
                 agg_groupby(df,
                             [primary_key],
                             {c:operation},
                             {c:primary_key+"_"+operation+"_"+c}),
                 primary_key)
  return df

# Lets create a new feature called height/diameter
flowers['ratio_height_diam']=np.where(flowers['Diameter']!=0,flowers['Height']/flowers['Diameter'],np.NAN)

aggregation_columns = ['Height','Diameter','MCA1','PCA1','ratio_height_diam']
operation=['mean','median','min','max','std']

# Analysing Area Code with Height,Diameter,MCA1,PCA1,ration_height_diam by using operation like mena ,median etc
for op in operation:
  flowers=aggregation(flowers,'Area_Code',op,aggregation_columns)

# Analysing Locality Code with Height,Diameter,MCA1,PCA1,ration_height_diam by using operation like mena ,median etc
for op in operation:
  flowers=aggregation(flowers,'Locality_Code',op,aggregation_columns)

# Analysing Region_Code with Height,Diameter,MCA1,PCA1,ration_height_diam by using operation like mena ,median etc
for op in operation:
  flowers=aggregation(flowers,'Region_Code',op,aggregation_columns)

# Analysing Species with Height,Diameter,MCA1,PCA1,ration_height_diam by using operation like mena ,median etc
for op in operation:
  flowers=aggregation(flowers,'Species',op,aggregation_columns)

"""> Selection of primary key:- Select a categorical column on which you wish to group the data. Say I want to analyse the data on the basis of all the unique area_codes in city..so area_code is primary key, or suppose on all unique species in city then species in primary key"""

print(len(flowers['Area_Code'].unique()))
print(len(flowers['Region_Code'].unique()))
print(len(flowers['Locality_Code'].unique()))
print(len(flowers['Species'].unique()))

"""> Till now we have choosen only numerical values like height,diameter to be aggreagte columns.

> We can also choose categorical column but we have to change our operation , we can apply mean or median. We use ooperation like nuinque , mode or frequency etc
"""

flowers=aggregation(flowers,'Area_Code','nunique',['Species']) # No.of unique species in area_codes
flowers=aggregation(flowers,'Region_Code','nunique',['Species']) # No.of unique species in region_codes
flowers=aggregation(flowers,'Locality_Code','nunique',['Species']) # No.of unique species in locality_codes
flowers=aggregation(flowers,'Area_Code','nunique',['Region_Code']) # No.of unique region_code in area_codess
flowers=aggregation(flowers,'Area_Code','nunique',['Locality_Code']) # No.of unique locality_code in area_codes
flowers=aggregation(flowers,'Region_Code','nunique',['Locality_Code']) # No.of unique locality_code in region_codes

flowers['Area_Code_nunique_Locality_Code'].value_counts()

flowers['Locality_Code'].unique()

"""# Data Preprocessing"""

train = flowers[:len(train)]
test  = flowers[len(train):]

for cols in categorical_cols:
    train[cols] = train[cols].astype(str)
    test[cols]  = test[cols].astype(str)

cate_features_index = np.where(train.dtypes == object)[0]

from sklearn.model_selection import  train_test_split
x_train,x_val,y_train,y_val=train_test_split(train,y,random_state=0,test_size=0.25)

"""# Catbbost"""

pip install catboost

from catboost import CatBoostClassifier
cat=CatBoostClassifier(n_estimators=2000,random_state=0)

cat.fit(x_train,y_train,cat_features=cate_features_index,eval_set=(x_val,y_val))

from sklearn.metrics import log_loss
log_loss(pd.get_dummies(y_val),cat.predict_proba(x_val))

cat_final=CatBoostClassifier(n_estimators=900,random_state=0)

cat_final.fit(train,y,cat_features=cate_features_index)

prob=cat.predict_proba(test)

result=pd.DataFrame(prob,columns=['Class0','Class1','Class2','Class3','Class4','Class5','Class6','Class7'])

result

class_pred=cat_final.predict(test)

result['Predicted_Class']=class_pred

result

